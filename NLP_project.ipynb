{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a57e68",
   "metadata": {},
   "source": [
    "# 1. Question\n",
    "What were the major positive and negative topics the youtube community discussed in response to Buttery Bros last 100 videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12b467",
   "metadata": {},
   "source": [
    "# 2. Collect and Clean Data\n",
    "The data will come from the comments left under the youtube vloggers last 100 videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c9924",
   "metadata": {},
   "source": [
    "## youtube API\n",
    "1. Enable API and create credentials. Login at developers.google.com and navigate to developer dashboard. Create new project and click \"Enable API Option\". search for and select Youtube Data API, then enable it. Select credentials from the menu on the right, \n",
    "\n",
    "2. Install google API client using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install (--upgrade) google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28aa7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import googleapiclient.discovery\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import cleantext #pypi.org/project/cleantext\n",
    "\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516df369",
   "metadata": {},
   "source": [
    "3. write python script to pull comments dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Sample Python code for youtube.commentThreads.list\n",
    "# See instructions for running these code samples locally:\n",
    "# https://developers.google.com/explorer-help/code-samples#python\n",
    "\n",
    "def yt_commentsV2(video_ID):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    #os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    DEVELOPER_KEY = \"AIzaSyB2-a0LYidQuaTjWalFVUe-uY_kl1p49n0\"\n",
    "    video_id = video_ID\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\", #snippet indicates the top comment and not the replies\n",
    "        videoId=video_id #identifies which video I want comments for\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    comments = []\n",
    "    \n",
    "    def ext_com(response):\n",
    "        coms = []\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "            coms.append(comment)\n",
    "            return coms\n",
    "\n",
    "    comments.extend(ext_com(response)) #extract comments from first page before checking for \"nextPageToken\"\n",
    "        \n",
    "    while response: #iterate through until no next page token or quota fills\n",
    "        \n",
    "        #check for next page token, if present pull JSON for that new page and extract comments\n",
    "        if 'nextPageToken' in response:\n",
    "            request2 =  youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken = response['nextPageToken'])\n",
    "            \n",
    "            response = request2.execute()\n",
    "            \n",
    "            comments.extend(ext_com(response))\n",
    "    \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230edb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 1-10\n",
    "# vid_IDs = ['pZmz39oz4IE',\n",
    "# '2gGGtCGjM2w',\n",
    "# 'TuqmrA1EWFY',\n",
    "# 'RsVo5Z0-7ng',\n",
    "# 'QfwqUbqv_Jw',\n",
    "# 'wA3I7egoGKI',\n",
    "# 'Rj2m8v-U_mk',\n",
    "# 'LqU4dMuHVC4',\n",
    "# 'ovQdumwflKo',\n",
    "# '1_2zOoaXwhU'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 1 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 11-20\n",
    "\n",
    "# vid_IDs = ['ppT1ioZEyHQ',\n",
    "# '8agePKOPQQE',\n",
    "# '_fHqcLgOmoU',\n",
    "# 'QyBsOSjnr5w',\n",
    "# 'kfV8rsD5BPc',\n",
    "# 'I1GSjmoSBMQ',\n",
    "# 'G2_MEUxpmI8',\n",
    "# 'UHeSNrYO1QM',\n",
    "# 'dVn2-OCdX-k',\n",
    "# 'h_DaPK9DkvU'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 11 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 21-30\n",
    "\n",
    "# vid_IDs = ['ylEPfEEK6Og',\n",
    "# '6PGqoXqPKWg',\n",
    "# 'rQBb0cDi9KE',\n",
    "# 'C6Zc7UQyghI',\n",
    "# 'K-5zutw2NpM',\n",
    "# 'nRNhpuJ8d9s',\n",
    "# 'V35cC2jG8sg',\n",
    "# 'pSFFanRnbeo',\n",
    "# '_Jj8XQ4xjDE',\n",
    "# 'yLMTBZ5A8g0'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 21 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012de7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 31-40\n",
    "\n",
    "# vid_IDs = ['VGUxuQdWP90',\n",
    "# 'jbl9MVOz4tk',\n",
    "# '50Ji51WYmxA',\n",
    "# 'Kr-HNt0fOJ0',\n",
    "# 'xUMWES-cbKs',\n",
    "# 'HHNOKecslsE',\n",
    "# 'FW5uH01sdcc',\n",
    "# 'd_2unYe7k1c',\n",
    "# '3nmwiDjPvWc',\n",
    "# 'QtSN8FWlG2s'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 31 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c72c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 41-50\n",
    "\n",
    "# vid_IDs = ['9x79HOMqouo',\n",
    "# '_mx6DlGShT8',\n",
    "# '46EKVvuCFus',\n",
    "# 'kYpzmi13roA',\n",
    "# '0RHDEkDD5H8',\n",
    "# 'SqEVmcLMxAw',\n",
    "# 'YZzy0foLVsw',\n",
    "# 'le4Z1Sv4u98',\n",
    "# 'zjLi49bE6jc',\n",
    "# 'djzcZaq04iQ',\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 41 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae48947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 51-60\n",
    "\n",
    "# vid_IDs = ['xsbKxsefUU0',\n",
    "# 'Q4F9QUc-cj8',\n",
    "# 'kiPvatTM6ps',\n",
    "# 'epeHiWA9884',\n",
    "# 'y5FUdu-CJ5w',\n",
    "# 'm6LelGa47nI',\n",
    "# 'gOLJ_lqHWpo',\n",
    "# 'kDNwLvlfilY',\n",
    "# 'QofThK2fTnQ',\n",
    "# 'naYOmsPHO-E'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 51 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 61-70\n",
    "\n",
    "# vid_IDs = ['ygyEXXZZ18Q',\n",
    "# 'G6wLGZw_dFg',\n",
    "# '2emfp-_b5d0',\n",
    "# '2raX7XqyQdI',\n",
    "# 'zypSaHPJCgc',\n",
    "# 'K9DMAm-_zk0',\n",
    "# '1Yt6zSHkQKo',\n",
    "# 'wmbIlknvGUA',\n",
    "# 'W3Ybke3JuSw',\n",
    "# 'eA1-jCY2_k4'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 61 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300132c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #videos 71-80\n",
    "\n",
    "# vid_IDs = ['ejw7M3A8t7I',\n",
    "# 'hBAJAV2WJ4k',\n",
    "# 'mhW3cUYalXs',\n",
    "# 'ysCl4cjSltM',\n",
    "# 'z7YhfoYDnyM',\n",
    "# 'MUAu12SuaPI',\n",
    "# '0mhJ3OzB-Nc',\n",
    "# 'g0h3YcDYFnY',\n",
    "# 'DZQIBUXRK8c',\n",
    "# 'YCHHE4jE2qk'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 71 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9479f",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# #videos 81-90\n",
    "\n",
    "# vid_IDs = ['BJ4fxKMGGrc',\n",
    "# 'YjBeDMb5yoI',\n",
    "# 'lPdw1ztxBck',\n",
    "# 'yycQPcCLvbo',\n",
    "# 'GAx9dbgHHLg',\n",
    "# 'FhPYP9-pIY4',\n",
    "# 'nibBF2U51-U',\n",
    "# 'r7sq8IS_1Is',\n",
    "# 'JzsPDv8tcXI',\n",
    "# 'mJSzI6Mpqcg'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 81 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48902256",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# #videos 91-100\n",
    "\n",
    "# vid_IDs = ['hEPkBSy052o',\n",
    "# 'd3VO399v-4w',\n",
    "# '00MHLTTUJtg',\n",
    "# 'fx6Ou1qXT2U',\n",
    "# 'h2Rf9ZEEZ9Q',\n",
    "# '2FN2gzRv55Y',\n",
    "# 'YIDdGw6pnVs',\n",
    "# 'POLo0nvprT4',\n",
    "# 'O9iEjv0blc4',\n",
    "# 'ix22fp-GXVg'\n",
    "# ] \n",
    "\n",
    "# vid_cnt = 91 #index variable for naming convention\n",
    "\n",
    "# for vid in vid_IDs:    \n",
    "#     comms = yt_commentsV2(vid) #extract youtube comments from vid and store in list\n",
    "#     df = pd.DataFrame(comms) #create dataframe of comments\n",
    "#     title = 'vid'+str(vid_cnt)+'_comments.csv' #create string title using index variable\n",
    "#     df.to_csv(title) #save comments dataframe to csv\n",
    "#     vid_cnt +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc856bf7",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d19bd7",
   "metadata": {},
   "source": [
    "Import compiled csv of all comments and get number of documents and total number of space separated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('comments.csv')\n",
    "corpus = comments.iloc[:,1]\n",
    "print('Num of Documents: ' + str(len(corpus)))\n",
    "print('Num of tokens: ' + str(sum([len(d.split(' ')) for d in corpus])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11103961",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head() #uppercase, punctuation, extra spaces all need to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ca75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text by removing: extra white space, to lowercase, digits, punctation, emotes, english stop words\n",
    "from cleantext import clean\n",
    "\n",
    "index = 0 #tracker int for iterating through corpus\n",
    "\n",
    "for doc in corpus:\n",
    "    corpus.iloc[index] = clean(doc, #replace doc in corpus with cleaned doc\n",
    "                                     extra_spaces = True, #remove extra spaces\n",
    "                                      stopwords = True, #remove english stop words\n",
    "                                      lowercase = True, #convert all to lowercase\n",
    "                                      numbers = True, #remove numbers\n",
    "                                      punct = True, #remove punctuation\n",
    "                                      stp_lang = 'english' #identify english as stop word language\n",
    "                                     )\n",
    "    index +=1 #iterate the index counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f989be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further clean text by removing emotes\n",
    "#regex and replace all unicode that is not 0041-007A with \"\"\n",
    "import re\n",
    "#cl_corp = corpus\n",
    "index = 0\n",
    "for doc in corpus:\n",
    "    corpus[index] = re.sub('[^A-Za-z ]+', '', doc) #use regex to find all characters not english alphabet and replace with ''\n",
    "    index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f96626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out all rows that are null\n",
    "new_corpus = corpus[~corpus.isnull()]\n",
    "print(len(corpus))\n",
    "print(len(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab02e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out all docs that are ''\n",
    "new_corpus = corpus[(corpus!='')]\n",
    "print(len(corpus))\n",
    "print(len(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a156526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect spelling and fix\n",
    "from textblob import TextBlob\n",
    "\n",
    "index = 0\n",
    "sc_corpus = new_corpus\n",
    "\n",
    "for doc in new_corpus:\n",
    "    b = TextBlob(doc)\n",
    "    result = b.correct()\n",
    "    sc_corpus[index] = result\n",
    "    index += 1\n",
    "    \n",
    "sc_corpus.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_corpus = sc_corpus\n",
    "index = 0\n",
    "\n",
    "for doc in sc_corpus:\n",
    "    fin_corpus[index] = str(doc)\n",
    "    index+=1\n",
    "    \n",
    "fin_corpus = pd.DataFrame(fin_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6745c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_corpus.head() #preprocessing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('corpus.pickle', 'wb') as f:\n",
    "#    pickle.dump(fin_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech tagging\n",
    "# only want nouns to reduce features\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#pos_tag takes in tokenized list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee7dc5",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "Tokenize into words and 2 word ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e654567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('corpus.pickle', 'rb') as g:\n",
    "    corpus = pickle.load(g)\n",
    "\n",
    "corpus = corpus['0'] #convert from dataframe to just series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lst = corpus.tolist() #convert series to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea98431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [random, comment, win, year, subscription, good]\n",
       "1                   [best, parts, commercial, history]\n",
       "2                           [socks, flipflops, normal]\n",
       "3                                    [random, comment]\n",
       "4    [loved, seller, working, celebration, skill, c...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert docs to list of words\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "index = 0 #tracker int for iterating through corpus\n",
    "\n",
    "tok_corpus = corpus\n",
    "for doc in corpus:\n",
    "    tok_corpus.iloc[index] = word_tokenize(doc)\n",
    "    index +=1 #iterate the index counter\n",
    "    \n",
    "tok_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece933b",
   "metadata": {},
   "source": [
    "### Part of Speech tagging\n",
    "Takes in tokenized list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d7e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech tagging\n",
    "# only want nouns to reduce features\n",
    "from nltk.tag import pos_tag\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_corpus = []\n",
    "for doc in tok_corpus:\n",
    "    pos_tokens = pos_tag(doc)\n",
    "    pos_corpus.append(pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc01bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403\n",
      "1273\n"
     ]
    }
   ],
   "source": [
    "#filter out all non-nouns (NN)\n",
    "\n",
    "noun_corpus = []\n",
    "\n",
    "for doc in pos_corpus:\n",
    "    doc_terms = []\n",
    "    for term in doc:\n",
    "        if term[1] == 'NN':\n",
    "            doc_terms.append(term[0])\n",
    "    noun_corpus.append(doc_terms)\n",
    "    \n",
    "#filter out empty docs that didn't have nouns\n",
    "fin_noun_corpus = []\n",
    "for doc in noun_corpus:\n",
    "    if doc != []:\n",
    "        fin_noun_corpus.append(doc)\n",
    "        \n",
    "print(len(noun_corpus))\n",
    "print(len(fin_noun_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dae0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save noun_corpus to csv\n",
    "#noun_list = pd.DataFrame(fin_noun_corpus)\n",
    "#noun_list.to_csv('noun_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aecc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of unique words for review\n",
    "\n",
    "def unique_terms(corpus):\n",
    "    uniq_list = []\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            if word not in uniq_list:\n",
    "                uniq_list.append(word)\n",
    "    return uniq_list\n",
    "\n",
    "uniq = unique_terms(fin_noun_corpus)\n",
    "print(len(uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of unique terms to .csv file so it can be reviewed in whole in excel\n",
    "#u_list = pd.DataFrame(uniq_list)\n",
    "#u_list.to_csv('unique_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-word ngrams corpus\n",
    "from nltk.util import ngrams\n",
    "\n",
    "index = 0 #tracker int for iterating through corpus\n",
    "ngram_corpus = corpus\n",
    "\n",
    "for doc in corpus:\n",
    "    ngram_corpus.iloc[index] = list(ngrams(doc,2))\n",
    "    index +=1 #iterate the index counter\n",
    "    \n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1e28c",
   "metadata": {},
   "source": [
    "# 2. Topic Modeling\n",
    "Vectorize and and generate topics with LSA and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119be41c",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "972f775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize with count vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#un-tokenize corpus so it can be entered into transform\n",
    "corp = []\n",
    "ind=0\n",
    "while ind <= len(fin_noun_corpus)-1:\n",
    "    corp.append(' '.join(fin_noun_corpus[ind]))\n",
    "    ind+=1\n",
    "\n",
    "cv = CountVectorizer()\n",
    "doc_term_matrix = cv.fit_transform(corp)\n",
    "\n",
    "lsa = TruncatedSVD(10)\n",
    "lsa.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c17cb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_mat = lsa.components_ #the v matrix, rows are topics/columns are tokens/ values are presence of tokens in topic\n",
    "terms = cv.get_feature_names() # for list of tokens and index to see which are in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aca4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def topic_terms_csv(v_mat, terms, topic_ind, title): #(lsa v-matrix, int index for topic, string title for csv file)\n",
    "    #get top n terms for each topic\n",
    "    topic = np.argsort(v_mat[topic_ind])[-10:] #sort array low to high, take highest 20 indices and store in top1\n",
    "    #print top n terms for each topic\n",
    "    topic_terms = []\n",
    "    for x in topic:\n",
    "        topic_terms.append(terms[x])\n",
    "    df = pd.DataFrame(topic_terms)\n",
    "    df.to_csv(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c64104a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 topics\n",
    "for x in range(0,10,1):\n",
    "    title = 'topic_'+str(x)+'_LSA.csv'\n",
    "    topic_terms_csv(v_mat, terms, x, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8503f56",
   "metadata": {},
   "source": [
    "V-Matrix is compared for 10-15-20 topic LSA using excel. File 'LSA_topic_comparison'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsa.transform(doc_term_matrix) # U matrix, rows are docs/ columns are topics/values are relative presence of topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c54cb",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b168aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtaz/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    }
   ],
   "source": [
    "# CV and doc_term_matrix are the same\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "#doc_term_matrix = cv.fit_transform(corpus_lst)\n",
    "\n",
    "nmf = NMF(20)\n",
    "nmf.fit(doc_term_matrix)\n",
    "\n",
    "h_mat = nmf.components_ #H matrix rows are topics, columns are terms, values are presence of term in topic\n",
    "nmf_terms = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67c12367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,20,1):\n",
    "    title = 'topic_'+str(x)+'_NMF.csv'\n",
    "    topic_terms_csv(h_mat, nmf_terms, x, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4a17e",
   "metadata": {},
   "source": [
    "# 3. Communicate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01beb49",
   "metadata": {},
   "source": [
    "See powerpoint and paper for results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(metis)",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
